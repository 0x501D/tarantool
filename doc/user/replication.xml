<!DOCTYPE book [
<!ENTITY % tnt SYSTEM "../tnt.ent">
%tnt;
]>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xml:id="replication">

<title>Replication</title>
<blockquote><para>
  To set up replication, it's necessary to prepare the master,
  configure a replica, and establish procedures for recovery from
  a degraded state.
</para></blockquote>

<section xml:id="replication-architecture">
    <title>Replication architecture</title>
  <para>
    A replica gets all updates from the master by continuously
    fetching and applying its write-ahead log (WAL).
    Each record in the WAL represents a single Tarantool
    command such as INSERT or UPDATE or DELETE, and is assigned
    a monotonically growing log sequence number (LSN).
    In essence, Tarantool replication is row-based:
    all data change commands are fully deterministic and operate
    on a single record.
  </para>
  <para>
    A stored program invocation
    <!-- , unless requested explicitly, -->
    is not written to the write-ahead log. Instead, log events
    for actual UPDATEs and DELETEs, performed by the Lua code,
    are written to the log. This ensures that possible
    non-determinism of Lua does not cause replication
    to go out of sync.
  </para>
<!--
  <para>
    It is still sometimes necessary to replicate stored program
    CALLs, rather than their effects: for example, when the
    procedure is fully deterministic and CALL representation in
    the WAL is known to be significantly more compact. Another
    example would be when a procedure is written to do one thing
    on the master, and another on a replica. BOX_RPL_STMT
    flag of the binary protocol can  be used 
    to replicate CALLs as statements.
  </para>
-->
  <para>
    For replication to work correctly, the latest LSN
    on the replica must match or fall behind the latest LSN
    on the master. If the replica had its own updates,
    this would lead to it getting out of sync, since
    updates from the master having identical LSNs would
    not be applied. In fact, if replication is ON, Tarantool
    does not accept updates, even on its <olink
    targetptr="primary_port">"listen" address</olink>.
  </para>
</section>

<section xml:id="setting-up-the-master">
  <title>Setting up the master</title>
  <para>
    To prepare the master for connections from the replica, it's only
    necessary to include "listen" in the initil <code>box.cfg</code>
    request, for example <code>box.cfg{listen=3301}</code>.
    A master with enabled "listen" uri can accept connections
    from as many replicas as necessary on that uri. Each replica
    has its own replication state.
  </para>
</section>
<section xml:id="settin-up-a-replica">
  <title>Setting up a replica</title>
  <para>
    A server, whether master or replica, always requires a valid
    snapshot file to boot from. For a master, a snapshot file is usually
    prepared as soon as <code>box.cfg</code> occurs.
    For a replica, it's usually copied from the master.
  </para>
  <para>
    To start replication, configure <olink
    targetptr="replication_source"/>.
    Other parameters can also be changed, but existing spaces and
    their primary keys on the replica must be identical to the ones on the
    master.
  </para>
  <para>
    Once connected to the master, the replica requests all changes
    that happened after the latest local LSN. It is therefore
    necessary to keep WAL files on the master host as long as
    there are replicas that haven't applied them yet. An example
    configuration can be found in <link
    xlink:href="https://github.com/tarantool/tarantool/blob/master/test/replication/cfg/replica.cfg"><filename>test/replication/cfg/replica.cfg</filename></link>.
  </para>
  <para>
    If required WAL files are absent, a replica can be "re-seeded" at
    any time with a newer snapshot file, manually copied from the
    master.
  </para>
  <note><simpara>
    Replication parameters are "dynamic", which allows the
    replica to become a master and vice versa with the help of the
    <olink targetptr="box.cfg">box.cfg</olink> statement.
  </simpara></note>

</section>
<section xml:id="recovering-from-a-degraded-state">
  <title>Recovering from a degraded state</title>
  <para>
    "Degraded state" is a situation when the master becomes
    unavailable -- due to hardware or network failure, or due to a
    programming bug. There is no reliable way for a replica to detect
    that the master is gone for good, since sources of failure and
    replication environments vary significantly.
  </para>
  <para>
    A separate monitoring script (or scripts, if a decision-making
    quorum is desirable) is necessary to detect a master failure.
    Such a script would typically try to update a tuple in an
    auxiliary space on the master, and raise an alarm if a
    network or disk error persists for longer than is acceptable.
  </para>
  <para>
    When a master failure is detected, the following needs
    to be done:
    <itemizedlist>
      <listitem>
        <para>
          First and foremost, make sure that the master does not
          accept updates. This is necessary to prevent the
          situation when, should the master failure end up being
          transient, some updates still go to the master, while
          others already end up on the replica.
        </para>
        <para>
          If the master is available, the easiest way to turn
          on read-only mode is to turn Tarantool into a replica of
          itself. This can be done by setting the master's <olink
          targetptr="replication_source"/> to point to self.
        </para>
        <para>
          If the master is not available, best bet is to log into
          the machine and kill the server, or change the
          machine's network configuration (DNS, IP address).
        </para>
        <para>
          If the machine is not available, it's perhaps prudent
          to power it off.
        </para>
      </listitem>
      <listitem>
        <para>
          Record the replica's LSN, by issuing <olink
          targetptr="box.info"/>. This LSN may prove useful if
          there are updates on the master that never reached
          the replica.
        </para>
      </listitem>
      <listitem>
        <para>
          Propagate the replica to become a master. This is done
          by setting <olink targetptr="replication_source"/>
          on replica to an empty string.
        </para>
      </listitem>
      <listitem>
        <para>
          Change the application configuration to point to the new
          master. This can be done either by changing the
          application's internal routing table, or by setting up
          the old master's IP address on the new master's machine, or
          using some other approach.
        </para>
      </listitem>
      <listitem>
        <para>
          Recover the old master. If there are updates that didn't
          make it to the new master, they have to be applied
          manually. You can use the Tarantool command line client
          to read the server log files.
        </para>
      </listitem>
    </itemizedlist>
  </para>
  
<para>
Replication allows multiple Tarantool servers to work on
copies of the same databases. The databases are kept in
synch because each server can communicate its changes to
all the other servers. Servers which share the same databases
are a "cluster".
</para>


<para>
 <bridgehead renderas="sect4">Instructions for quick startup of a new two-server simple cluster</bridgehead>
Step 1. Start the first server thus:<programlisting><userinput>box.cfg{listen=<replaceable>uri#1</replaceable>}</userinput>
<userinput>box.schema.user.grant('guest','read,write,execute','universe') -- replace with more restrictive request</userinput>
<userinput>box.snapshot()</userinput></programlisting>... Now a new cluster exists.
</para>
<para>
Step 2. Check where the second server's files will go by looking at
   its directories (snap_dir, wal_dir). They must be empty --
   when the second server joins for the first time, it has to
   be working with a clean slate so that the initial copy of
   the first server's databases can happen without conflicts.
</para>
<para>
Step 3. Start the second server thus:<programlisting><userinput>box.cfg{listen=<replaceable>uri#2</replaceable>, replication_source=<replaceable>uri#1</replaceable>}</userinput></programlisting>
... where uri#1 = the primary port that the first server is listening on.
</para>
<para>
That's all.
</para>
<para>
In this configuration, the first server is the "master" and
the second server is the "replica". Henceforth every change
that happens on the master will be visible on the replica.
A simple two-server cluster with the master on one computer
and the replica on a different computer is very common and
provides two benefits: FAILOVER (because if the master goes
down then the replica can take over), or LOAD BALANCING
(because clients can connect to either the master or the
replica for select requests).
</para>

  <para>
  <bridgehead renderas="sect4">Master-master</bridgehead>
  In the simple master-replica configuration, the master's
  changes are seen by the replica, but not vice versa,
  because the master was specified as the sole replication source.
  Starting with Tarantool 1.6, it's possible to go both ways.
  Starting with the simple configuration, the first server has to say:
  <code>box.cfg{replication_source=<replaceable>uri#2</replaceable>}</code>.
  This request can be performed at any time.
  </para>
  <para>
  In this configuration, both servers are "masters" and
  both servers are "replicas". Henceforth every change
  that happens on either server will be visible on the other.
  The failover benefit is still present, and the load-balancing
  benefit is enhanced (because clients can connect to either
  server for data-change requests as well as select requests).
  </para>
  <para>
  If two operations for the same tuple take place "concurrently"
  (which can involve a long interval because replication is asynchronous),
  and one of the operations is <code>delete</code> or <code>replace</code>,
  there is a possibility that servers will end up with different
  contents.
  </para>
  <para>
  <bridgehead renderas="sect4">All the "What If?" Questions</bridgehead>
  <emphasis>What if there are more than two servers with master-master?</emphasis>
  ... On each server, specify the replication_source for all
  the others. For example, server #3 would have a request:
  <code>box.cfg{replication_source=<replaceable>uri#1</replaceable>, replication_source=<replaceable>uri#2</replaceable>}</code>.
  </para>
  <para>
  <emphasis>What if a a server should be taken out of the cluster?</emphasis>
  ... Run box.cfg{} again specifying a blank replication source:
  <code>box.cfg{replication_source=''}</code>.
  </para>
  <para>
  <emphasis>What if a server leaves the cluster?</emphasis>
  ... The other servers carry on. If the wayward server rejoins,
  it will receive all the updates that the other servers made
  while it was away.
  </para>
  <para>
  <emphasis>What if two servers both change the same tuple?</emphasis>
  ... The last changer wins. For example, suppose that server#1 changes
  the tuple, then server#2 changes the tuple. In that case server#2's
  change overrides whatever server#1 did. In order to 
  keep track of who came last, Tarantool implements a
  <link xlink:href="https://en.wikipedia.org/wiki/Vector_clock">vector clock</link>.
  </para>
  <para>
  <emphasis>What if a master disappears and the replica must take over?</emphasis>
  ... A message will appear on the replica stating that the
  connection is lost. The replica must now become independent,
  which can be done by saying
  <code>box.cfg{replication_source=''}</code>.
  </para>
  <para>
  <emphasis>What if it's necessary to know what cluster a server is in?</emphasis>
  ... The identification of the cluster is a UUID which is generated
  when the first master starts for the first time. This UUID is
  stored in the system space <code>_cluster</code>, in the first tuple. So to
  see it, say:
  <code>box.space._cluster:select{1}</code>
  </para>
  <para>
  <emphasis>What if one of the server's files is corrupted or deleted?</emphasis>
  ... Stop the other servers, copy all the database files (the
  ones with extension "snap" or "xlog") over to the server with
  the problem, and then restart all servers.
  </para>

  <para>
  <bridgehead renderas="sect4">Hands-On (Tutorial)</bridgehead>
  After following the steps here,
  an administrator will have experience
  creating a cluster and adding a replica.
  </para>
  <para>
  Start two shells. Put them side by side on the screen.
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry>__________TERMINAL #1__________</entry><entry>__________TERMINAL #2__________</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting><prompt>$</prompt></programlisting></entry>
          <entry><programlisting><prompt>$</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>
  On the first shell, which we'll call Terminal #1,
  execute these commands:
<programlisting>
<userinput># Terminal 1</userinput>
<userinput>mkdir -p ~/tarantool_test_node_1</userinput>
<userinput>cd ~/tarantool_test_node_1</userinput>
<userinput>rm -R ~/tarantool_test_node_1/*</userinput>
<userinput>~/tarantool-master/src/tarantool</userinput>
<userinput>box.cfg{listen=3301, logger='filename.log'}</userinput>
<userinput>box.schema.user.grant('guest','read,write,execute','universe')</userinput>
<userinput>box.space._cluster:select({0},{iterator='GE'})</userinput>
</programlisting>
</para>
<para>
The result is that a new cluster is set up, and
the UUID is displayed.
Now the screen looks like this: (except that UUID values are always different):
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>$ <userinput># Terminal 1</userinput>
$ <userinput>mkdir -p ~/tarantool_test_node_1</userinput>
$ <userinput>cd ~/tarantool_test_node_1</userinput>
~/tarantool_test_node_1$ <userinput>rm -R ~/tarantool_test_node_1/*</userinput>
~/tarantool_test_node_1$ <userinput>~/tarantool-master/src/tarantool</userinput>
~/tarantool-master/src/tarantool: version 1.6.0-1724-g033ed69
type 'help' for interactive help
tarantool&gt; <userinput>box.cfg{listen=3301}</userinput>
... ...
tarantool&gt; <userinput>box.schema.user.grant('guest','read,write,execute','universe')</userinput>
2014-08-14 13:39:57.712 [24956] wal I> creating `./00000000000000000000.xlog.inprogress'
---
...
tarantool&gt; <userinput>box.space._cluster:select({0},{iterator='GE'})</userinput>
---
- - [1, 'd3de1435-5e26-4122-95e5-3e2d40e6e1df']
...
</programlisting></entry>
          <entry><programlisting>$














</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

On the second shell, which we'll call Terminal #2,
execute these commands:<programlisting>
<userinput># Terminal 2</userinput>
<userinput>mkdir -p ~/tarantool_test_node_2</userinput>
<userinput>cd ~/tarantool_test_node_2</userinput>
<userinput>rm -R ~/tarantool_test_node_2/*</userinput>
<userinput>~/tarantool-master/src/tarantool</userinput>
<userinput>box.cfg{listen=3302, replication_source=3301}</userinput>
<userinput>box.space._cluster:select({0},{iterator='GE'})</userinput></programlisting>
The result is that a replica is set up.
Messages appear on Terminal #1 confirming that the
replica has connected and that the WAL contents
have been shipped to the replica.
Messages appear on Terminal #2 showing that
replication is starting.
Also on Terminal#2 the _cluster UUID value is displayed, and it is
the same as the _cluster UUID value that
was displayed on Terminal #1, because both
servers are in the same cluster.

    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; box.space._cluster:select({0},{iterator='GE'})
---
- - [1, 'd3de1435-5e26-4122-95e5-3e2d40e6e1df']
...
tarantool> 2014-08-14 13:41:31.097 [24958] main/101/spawner I> created a replication relay: pid = 25148
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> recovery start
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> recovering from `./00000000000000000000.snap'
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> snapshot sent
2014-08-14 13:41:31.190 [24958] main/101/spawner I> created a replication relay: pid = 25150
2014-08-14 13:41:31.291 [25150] main/101/relay/127.0.0.1:42759 I> recover from `./00000000000000000000.xlog'</programlisting></entry>
<entry><programlisting><prompt>$</prompt> <userinput># Terminal 2</userinput>
~/tarantool_test_node_2$ <userinput>mkdir -p ~/tarantool_test_node_2</userinput>
~/tarantool_test_node_2$ <userinput>cd ~/tarantool_test_node_2</userinput>
~/tarantool_test_node_2$ <userinput>rm -R ~/tarantool_test_node_2/*</userinput>
~/tarantool_test_node_2$ <userinput>~/tarantool-master/src/tarantool</userinput>
/home/pgulutzan/tarantool-master/src/tarantool: version 1.6.0-1724-g033ed69
type 'help' for interactive help
tarantool&gt; <userinput>box.cfg{listen=3302, replication_source=3301}</userinput>
... ...
---
...
tarantool&gt; <userinput>box.space._cluster:select({0},{iterator='GE'})</userinput>
2014-08-14 13:41:31.189 [25139] main/102/replica/0.0.0.0:3301 C> connected to master
2014-08-14 13:41:31.291 [25139] wal I> creating `./00000000000000000000.xlog.inprogress'
---
- - [1, 'd3de1435-5e26-4122-95e5-3e2d40e6e1df']
  - [2, 'ea7d17d7-6690-4334-b09c-f38ffa305d36']
...
</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

On Terminal #1, execute these requests:
<programlisting><userinput>s = box.schema.create_space('tester')</userinput>
<userinput>s:create_index('primary', {})</userinput>
<userinput>s:insert{1,'Tuple inserted on Terminal #1'}</userinput></programlisting>
Now the screen looks like this:
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; 2014-08-14 13:41:31.097 [24958] main/101/spawner I> created a replication relay: pid = 25148
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> recovery start
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> recovering from `./00000000000000000000.snap'
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> snapshot sent
2014-08-14 13:41:31.190 [24958] main/101/spawner I> created a replication relay: pid = 25150
2014-08-14 13:41:31.291 [25150] main/101/relay/127.0.0.1:42759 I> recover from `./00000000000000000000.xlog'
<userinput>s = box.schema.create_space('tester')</userinput>
---
...
tarantool&gt; <userinput>s:create_index('primary', {})</userinput>
---
...
tarantool&gt; <userinput>s:insert{1,'Tuple inserted on Terminal #1'}</userinput>
---
- [1, 'Tuple inserted on Terminal #1']
...
</programlisting></entry>
          <entry><programlisting><prompt>$ # Terminal 2
~/tarantool_test_node_2$ mkdir -p ~/tarantool_test_node_2
~/tarantool_test_node_2$ cd ~/tarantool_test_node_2
~/tarantool_test_node_2$ rm -R ~/tarantool_test_node_2/*
~/tarantool_test_node_2$ ~/tarantool-master/src/tarantool
/home/pgulutzan/tarantool-master/src/tarantool: version 1.6.0-1724-g033ed69
type 'help' for interactive help
tarantool&gt; box.cfg{listen=3302, replication_source=3301}
... ...
---
...
tarantool&gt; box.space._cluster:select({0},{iterator='GE'})
2014-08-14 13:41:31.189 [25139] main/102/replica/0.0.0.0:3301 C> connected to master
2014-08-14 13:41:31.291 [25139] wal I> creating `./00000000000000000000.xlog.inprogress'
---
- - [1, 'd3de1435-5e26-4122-95e5-3e2d40e6e1df']
  - [2, 'ea7d17d7-6690-4334-b09c-f38ffa305d36']
...</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

The creation and insertion were successful on Terminal #1.
Nothing has happened on Terminal #2.
</para>
<para>
On Terminal #2, execute these requests:<programlisting>
<userinput>s = box.space.tester</userinput>
<userinput>s:select({1},{iterator='GE'})</userinput>
<userinput>s:insert{2,'Tuple inserted on Terminal #2'}</userinput></programlisting>
Now the screen looks like this:

    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting><prompt>... ...
tarantool&gt; 2014-08-14 13:41:31.097 [24958] main/101/spawner I> created a replication relay: pid = 25148
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> recovery start
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> recovering from `./00000000000000000000.snap'
2014-08-14 13:41:31.098 [25148] main/101/relay/127.0.0.1:42758 I> snapshot sent
2014-08-14 13:41:31.190 [24958] main/101/spawner I> created a replication relay: pid = 25150
2014-08-14 13:41:31.291 [25150] main/101/relay/127.0.0.1:42759 I> recover from `./00000000000000000000.xlog'
s = box.schema.create_space('tester')
---
...
tarantool&gt; s:create_index('primary', {})
---
...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...</prompt></programlisting></entry>
          <entry><programlisting>... ...
tarantool&gt; box.space._cluster:select({0},{iterator='GE'})
2014-08-14 13:41:31.189 [25139] main/102/replica/0.0.0.0:3301 C> connected to master
2014-08-14 13:41:31.291 [25139] wal I> creating `./00000000000000000000.xlog.inprogress'
---
- - [1, 'd3de1435-5e26-4122-95e5-3e2d40e6e1df']
  - [2, 'ea7d17d7-6690-4334-b09c-f38ffa305d36']
...
tarantool&gt; <userinput>s = box.space.tester</userinput>
---
...
tarantool&gt; <userinput>s:select({1},{iterator='GE'})</userinput>
---
- - [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; <userinput>s:insert{2,'Tuple inserted on Terminal #2'}</userinput>
---
- [2, 'Tuple inserted on Terminal #2']
...
</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

The selection and insertion were successful on Terminal #2.
Nothing has happened on Terminal #1.
</para>
<para>
On Terminal #1, execute these Tarantool requests and shell commands:<programlisting>
<userinput>os.exit()</userinput>
<userinput>ls -l ~/tarantool_test_node_1</userinput>
<userinput>ls -l ~/tarantool_test_node_2</userinput></programlisting>
Now Tarantool #1 is stopped.
Messages appear on Terminal #2 announcing that fact.
The "ls -l" commands show that both servers have
made snapshots, which have the same size because
they both contain the same tuples.

    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; <userinput>os.exit()</userinput>
2014-08-14 15:08:40.376 [25150] main/101/relay/127.0.0.1:42759 I> done `./00000000000000000000.xlog'
2014-08-14 15:08:40.414 [24958] main/101/spawner I> Exiting: master shutdown
2014-08-14 15:08:40.414 [24958] main/101/spawner I> sending signal 15 to 1 children
2014-08-14 15:08:40.414 [24958] main/101/spawner I> waiting for children for up to 5 seconds
~/tarantool_test_node_1$ <userinput>ls -l ~/tarantool_test_node_1</userinput>
total 12
-rw-rw-r-- 1  1781 Aug 14 13:39 00000000000000000000.snap
-rw-rw-r-- 1   416 Aug 14 15:08 00000000000000000000.xlog
drwxr-x--- 2  4096 Aug 14 13:39 sophia
~/tarantool_test_node_1$ <userinput>ls -l ~/tarantool_test_node_2</userinput>
total 12
-rw-rw-r-- 1  1781 Aug 14 13:41 00000000000000000000.snap
-rw-rw-r-- 1   486 Aug 14 14:52 00000000000000000000.xlog
drwxr-x--- 2  4096 Aug 14 13:41 sophia
</programlisting></entry>
          <entry><programlisting><prompt>... ...
tarantool&gt; s:select({1},{iterator='GE'})
---
- - [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> can't read row
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 11, aka 127.0.0.1:42759, peer of 127.0.0.1:3301: Broken pipe
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> will retry every 1 second
</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

On Terminal #2, execute these requests:<programlisting>
<userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
<userinput>box.space.tester:insert{3,'Another'}</userinput></programlisting>
Now the screen looks like this:
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting><prompt>... ...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; os.exit()
2014-08-14 15:08:40.376 [25150] main/101/relay/127.0.0.1:42759 I> done `./00000000000000000000.xlog'
2014-08-14 15:08:40.414 [24958] main/101/spawner I> Exiting: master shutdown
2014-08-14 15:08:40.414 [24958] main/101/spawner I> sending signal 15 to 1 children
2014-08-14 15:08:40.414 [24958] main/101/spawner I> waiting for children for up to 5 seconds
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_1
total 12
-rw-rw-r-- 1  1781 Aug 14 13:39 00000000000000000000.snap
-rw-rw-r-- 1   416 Aug 14 15:08 00000000000000000000.xlog
drwxr-x--- 2  4096 Aug 14 13:39 sophia
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_2
total 12
-rw-rw-r-- 1  1781 Aug 14 13:41 00000000000000000000.snap
-rw-rw-r-- 1   486 Aug 14 14:52 00000000000000000000.xlog
drwxr-x--- 2  4096 Aug 14 13:41 sophia
</prompt></programlisting></entry>
          <entry><programlisting>... ...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> can't read row
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 11, aka 127.0.0.1:42759, peer of 127.0.0.1:3301: Broken pipe
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> will retry every 1 second
tarantool&gt; <userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
...

tarantool&gt; <userinput>box.space.tester:insert{3,'Another'}</userinput>
---
- [3, 'Another']
...
</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

Terminal #2 has done a select and an insert,
even though Terminal #1 is down.
</para>
<para>
On Terminal #1 execute these commands:<programlisting>
<userinput>~/tarantool-master/src/tarantool</userinput>
<userinput>box.cfg{listen=3301}</userinput>
<userinput>box.space.tester:select({0},{iterator='GE'})</userinput></programlisting>
Now the screen looks like this:
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; os.exit()
2014-08-14 15:08:40.376 [25150] main/101/relay/127.0.0.1:42759 I> done `./00000000000000000000.xlog'
2014-08-14 15:08:40.414 [24958] main/101/spawner I> Exiting: master shutdown
2014-08-14 15:08:40.414 [24958] main/101/spawner I> sending signal 15 to 1 children
2014-08-14 15:08:40.414 [24958] main/101/spawner I> waiting for children for up to 5 seconds
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_1
total 12
-rw-rw-r-- 1  1781 Aug 14 13:39 00000000000000000000.snap
-rw-rw-r-- 1   416 Aug 14 15:08 00000000000000000000.xlog
drwxr-x--- 2  4096 Aug 14 13:39 sophia
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_2
total 12
-rw-rw-r-- 1  1781 Aug 14 13:41 00000000000000000000.snap
-rw-rw-r-- 1   486 Aug 14 14:52 00000000000000000000.xlog
drwxr-x--- 2  4096 Aug 14 13:41 sophia
~/tarantool_test_node_1$ <userinput>~/tarantool-master/src/tarantool</userinput>
~/tarantool: version 1.6.0-1724-g033ed69
type 'help' for interactive help
tarantool&gt; <userinput>box.cfg{listen=3301}</userinput>
... ...
---
...
tarantool&gt; <userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
2014-08-14 15:22:22.883 [14305] main/101/spawner I> created a replication relay: pid = 14313
2014-08-14 15:22:22.983 [14313] main/101/relay/127.0.0.1:43646 I> recover from `./00000000000000000000.xlog'
2014-08-14 15:22:22.984 [14313] main/101/relay/127.0.0.1:43646 I> done `./00000000000000000000.xlog'
---
- - [1, 'Tuple inserted on Terminal #1']
...
</programlisting></entry>
          <entry><programlisting><prompt>... ...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> can't read row
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 11, aka 127.0.0.1:42759, peer of 127.0.0.1:3301: Broken pipe
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> will retry every 1 second
tarantool&gt; box.space.tester:select({0},{iterator='GE'})
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
...

tarantool&gt; box.space.tester:insert{3,'Another'}
---
- [3, 'Another']
...
tarantool&gt;
2014-08-14 15:22:22.881 [25139] main/102/replica/0.0.0.0:3301 C> connected to master
</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>


The master has reconnected to the cluster,
and has NOT found what the replica wrote
while the master was away. That is not a
surprise -- the replica has not been asked
to act as a replication source.
</para>
<para>
On Terminal #1, say:<programlisting>
<userinput>box.cfg{replication_source='3302'}</userinput>
<userinput>box.space.tester:select({0},{iterator='GE'})</userinput></programlisting>
The screen now looks like this:
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
~/tarantool_test_node_1$ ~/tarantool-master/src/tarantool
~/tarantool: version 1.6.0-1724-g033ed69
type 'help' for interactive help
tarantool&gt; box.cfg{listen=3301}
... ...
---
...
tarantool&gt; box.space.tester:select({0},{iterator='GE'})
2014-08-14 15:22:22.883 [14305] main/101/spawner I> created a replication relay: pid = 14313
2014-08-14 15:22:22.983 [14313] main/101/relay/127.0.0.1:43646 I> recover from `./00000000000000000000.xlog'
2014-08-14 15:22:22.984 [14313] main/101/relay/127.0.0.1:43646 I> done `./00000000000000000000.xlog'
---
- - [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; <userinput>box.cfg{replication_source='3302'}</userinput>
2014-08-14 15:35:47.567 [14303] main/101/interactive C> starting replication from 0.0.0.0:3302
---
...
tarantool&gt; <userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
2014-08-14 15:35:47.568 [14303] main/103/replica/0.0.0.0:3302 C> connected to master
2014-08-14 15:35:47.670 [14303] wal I> creating `./00000000000000000005.xlog.inprogress'
2014-08-14 15:35:47.684 [14313] main/101/relay/127.0.0.1:43646 I> recover from `./00000000000000000005.xlog'
</programlisting></entry>
          <entry><programlisting><prompt>... ...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> can't read row
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 11, aka 127.0.0.1:42759, peer of 127.0.0.1:3301: Broken pipe
2014-08-14 15:08:40.417 [25139] main/102/replica/0.0.0.0:3301 I> will retry every 1 second
tarantool&gt; box.space.tester:select({0},{iterator='GE'})
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
...

tarantool&gt; box.space.tester:insert{3,'Another'}
---
- [3, 'Another']
...
tarantool&gt;
2014-08-14 15:22:22.881 [25139] main/102/replica/0.0.0.0:3301 C> connected to master
tarantool&gt; 2014-08-14 15:35:47.569 [25141] main/101/spawner I> created a replication relay: pid = 15585
2014-08-14 15:35:47.670 [15585] main/101/relay/127.0.0.1:51915 I> recover from `./00000000000000000000.xlog'
</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

This shows that the two servers are once
again in synch, and that each server sees
what the other server wrote.
</para>
<para>
To clean up, say "os.exit()" on both
Terminal #1 and Terminal #2, and then
on either terminal say:<programlisting>
<userinput>cd ~</userinput>
<userinput>rm -R ~/tarantool_test_node_1</userinput>
<userinput>rm -R ~/tarantool_test_node_2</userinput></programlisting>
</para>


</section>

</chapter>

<!--
vim: tw=66 syntax=docbk
vim: spell spelllang=en_us
-->
