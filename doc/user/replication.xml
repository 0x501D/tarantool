<!DOCTYPE book [
<!ENTITY % tnt SYSTEM "../tnt.ent">
%tnt;
]>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xml:id="replication">

<title>Replication</title>

<para>
Replication allows multiple Tarantool servers to work on
copies of the same databases. The databases are kept in
synch because each server can communicate its changes to
all the other servers. Servers which share the same databases
are a "cluster". Each server in a cluster also has a numeric
identifier which is unique within the cluster, known as the
"server id".
</para>

<blockquote><para>
  To set up replication, it's necessary to set up the master
  servers which make the original data-change requests,
  set up the replica servers which copy data-change requests
  from masters, and establish procedures for recovery from
  a degraded state.
</para></blockquote>

<section xml:id="replication-architecture">
    <title>Replication architecture</title>
  <para>
    A replica gets all updates from the master by continuously
    fetching and applying its write-ahead log (WAL).
    Each record in the WAL represents a single Tarantool
    data-change request such as INSERT or UPDATE or DELETE, and is assigned
    a monotonically growing log sequence number (LSN).
    In essence, Tarantool replication is row-based:
    each data change command is fully deterministic and operate
    on a single tuple.
  </para>
  <para>
    A stored program invocation
    <!-- , unless requested explicitly, -->
    is not written to the write-ahead log. Instead, log events
    for actual data-change requests, performed by the Lua code,
    are written to the log. This ensures that possible
    non-determinism of Lua does not cause replication
    to go out of sync.
  </para>
<!--
  <para>
    It is still sometimes necessary to replicate stored program
    CALLs, rather than their effects: for example, when the
    procedure is fully deterministic and CALL representation in
    the WAL is known to be significantly more compact. Another
    example would be when a procedure is written to do one thing
    on the master, and another on a replica. BOX_RPL_STMT
    flag of the binary protocol can  be used 
    to replicate CALLs as statements.
  </para>
-->

</section>

<section xml:id="setting-up-the-master">
  <title>Setting up the master</title>
  <para>
    To prepare the master for connections from the replica, it's only
    necessary to include "listen" in the initial <code>box.cfg</code>
    request, for example <code>box.cfg{listen=3301}</code>.
    A master with enabled "listen" <link linkend="URI">URI</link> can accept connections
    from as many replicas as necessary on that URI. Each replica
    has its own replication state.
  </para>
</section>
<section xml:id="settin-up-a-replica">
  <title>Setting up a replica</title>
  <para>
    A server requires a valid snapshot (.snap) file.
    A snapshot file is created for a server the first time that
    <code>box.cfg</code> occurs for it.
    If this first <code>box.cfg</code> request occurs without
    a "replication_source" clause, then the server is a master
    and starts its own new cluster with a new unique UUID.
    If this first <code>box.cfg</code> request occurs with
    a "replication_source" clause, then the server is a replica
    and its snapshot file, along with the cluster information,
    is constructed from the write-ahead logs of the master. Therefore,
    to start replication, specify <olink
    targetptr="replication_source"/> in a <code>box.cfg</code> request.
    When a replica contacts a master for the first time, it becomes part of a cluster.
    On subsequent occasions, it should always contact a master in the same cluster.
  </para>
  <para>
    Once connected to the master, the replica requests all changes
    that happened after the latest local LSN. It is therefore
    necessary to keep WAL files on the master host as long as
    there are replicas that haven't applied them yet.
    A replica can be "re-seeded" by deleting all its files (the snapshot .snap file
    and the WAL .xlog files), then starting replication again -- the replica will
    then catch up with the master by retrieving all the master's tuples.
    Again, this procedure works only if the master's WAL files are present.
  </para>
  <note><simpara>
    Replication parameters are "dynamic", which allows the
    replica to become a master and vice versa with the help of the
    <olink targetptr="box.cfg">box.cfg</olink> statement.
  </simpara></note>
  <note><simpara>
    If the <link linkend="snapshot-daemon">snaapshot daemon</link>
    is running on the master, it should also be running on the replica.
  </simpara></note>

</section>
<section xml:id="recovering-from-a-degraded-state">
  <title>Recovering from a degraded state</title>
  <para>
    "Degraded state" is a situation when the master becomes
    unavailable -- due to hardware or network failure, or due to a
    programming bug. There is no automatic way for a replica to detect
    that the master is gone for good, since sources of failure and
    replication environments vary significantly.
    So the detection of degraded state requires a human inspection.
  </para>
  <para>
    However, once a master failure is detected, the recovery
    is simple: declare that the replica is now the new master,
    by saying <code>box.cfg{... listen=URI}</code>.
    Then, if there are updates on the old master that were not
    propagated before the old master went down, they would have
    to be re-applied manually.
  </para>
 </section>
 <section>
 <title>Instructions for quick startup of a new two-server simple cluster</title>
<para>
Step 1. Start the first server thus:<programlisting><userinput>box.cfg{listen=<replaceable>uri#1</replaceable>}</userinput>
<userinput>box.schema.user.grant('guest','read,write,execute','universe') -- replace with more restrictive request</userinput>
<userinput>box.snapshot()</userinput></programlisting>... Now a new cluster exists.
</para>
<para>
Step 2. Check where the second server's files will go by looking at
its directories (<olink targetptr="snap_dir">snap_dir</olink> for snapshot files,
<olink targetptr="wal_dir">wal_dir</olink> for .xlog files). They must be empty --
when the second server joins for the first time, it has to
be working with a clean slate so that the initial copy of
the first server's databases can happen without conflicts.
</para>
<para>
Step 3. Start the second server thus:<programlisting><userinput>box.cfg{listen=<replaceable>uri#2</replaceable>, replication_source=<replaceable>uri#1</replaceable>}</userinput></programlisting>
... where uri#1 = the primary port that the first server is listening on.
</para>
<para>
That's all.
</para>
<para>
In this configuration, the first server is the "master" and
the second server is the "replica". Henceforth every change
that happens on the master will be visible on the replica.
A simple two-server cluster with the master on one computer
and the replica on a different computer is very common and
provides two benefits: FAILOVER (because if the master goes
down then the replica can take over), or LOAD BALANCING
(because clients can connect to either the master or the
replica for select requests).
</para>
</section>

<section>
<title>Master-Master Replication</title>
<para>
  In the simple master-replica configuration, the master's
  changes are seen by the replica, but not vice versa,
  because the master was specified as the sole replication source.
  Starting with Tarantool 1.6, it's possible to go both ways.
  Starting with the simple configuration, the first server has to say:
  <code>box.cfg{replication_source=<replaceable>uri#2</replaceable>}</code>.
  This request can be performed at any time.
  </para>
  <para>
  In this configuration, both servers are "masters" and
  both servers are "replicas". Henceforth every change
  that happens on either server will be visible on the other.
  The failover benefit is still present, and the load-balancing
  benefit is enhanced (because clients can connect to either
  server for data-change requests as well as select requests).
  </para>
  <para>
  If two operations for the same tuple take place "concurrently"
  (which can involve a long interval because replication is asynchronous),
  and one of the operations is <code>delete</code> or <code>replace</code>,
  there is a possibility that servers will end up with different
  contents.
  </para>
</section>
<section>
  <title>All the "What If?" Questions</title>
  <para>
  <emphasis>What if there are more than two servers with master-master?</emphasis>
  ... On each server, specify the replication_source for all
  the others. For example, server #3 would have a request:
  <code>box.cfg{replication_source=<replaceable>uri#1</replaceable>, replication_source=<replaceable>uri#2</replaceable>}</code>.
  </para>
  <para>
  <emphasis>What if a a server should be taken out of the cluster?</emphasis>
  ... Run box.cfg{} again specifying a blank replication source:
  <code>box.cfg{replication_source=''}</code>.
  </para>
  <para>
  <emphasis>What if a server leaves the cluster?</emphasis>
  ... The other servers carry on. If the wayward server rejoins,
  it will receive all the updates that the other servers made
  while it was away.
  </para>
  <para>
  <emphasis>What if two servers both change the same tuple?</emphasis>
  ... The last changer wins. For example, suppose that server#1 changes
  the tuple, then server#2 changes the tuple. In that case server#2's
  change overrides whatever server#1 did. In order to 
  keep track of who came last, Tarantool implements a
  <link xlink:href="https://en.wikipedia.org/wiki/Vector_clock">vector clock</link>.
  </para>
  <para>
  <emphasis>What if a master disappears and the replica must take over?</emphasis>
  ... A message will appear on the replica stating that the
  connection is lost. The replica must now become independent,
  which can be done by saying
  <code>box.cfg{replication_source=''}</code>.
  </para>
  <para>
  <emphasis>What if it's necessary to know what cluster a server is in?</emphasis>
  ... The identification of the cluster is a UUID which is generated
  when the first master starts for the first time. This UUID is
  stored in a tuple of the _<code>_cluster</code> system space,
  and in a tuple of the <code>_schema</code> system space. So to
  see it, say:
  <code>box.space._schema:select{'cluster'}</code>
  </para>
  <para>
  <emphasis>What if one of the server's files is corrupted or deleted?</emphasis>
  ... Stop the server, destroy all the database files (the
  ones with extension "snap" or "xlog" or ".inprogress"),
  restart the server, and catch up with the master by contacting it again
  (just say <code>box.cfg{...replication_source=...}</code>).
  </para>
  <para>
  <emphasis>What if replication causes security concerns?</emphasis>
  ... Prevent unauthorized replication sources by associating a password
   with every user that has access privileges for the relevant spaces.
   That way, the <link linkend="URI">URI</link> for the replication_source parameter
   will always have to have the long form <code>replication_source='username:password@host:port'</code>.
  </para>
 </section>
 <section>
  <title>Hands-On Replication Tutorial</title>
  <para>
  After following the steps here,
  an administrator will have experience
  creating a cluster and adding a replica.
  </para>
  <para>
  Start two shells. Put them side by side on the screen.
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry>______________TERMINAL #1______________</entry><entry>______________TERMINAL #2______________</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting><prompt>$</prompt></programlisting></entry>
          <entry><programlisting><prompt>$</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>
  On the first shell, which we'll call Terminal #1,
  execute these commands:
<programlisting>
<userinput># Terminal 1</userinput>
<userinput>mkdir -p ~/tarantool_test_node_1</userinput>
<userinput>cd ~/tarantool_test_node_1</userinput>
<userinput>rm -R ~/tarantool_test_node_1/*</userinput>
<userinput>~/tarantool-master/src/tarantool</userinput>
<userinput>box.cfg{listen=3301}</userinput>
<userinput>box.schema.user.create('replication', {password = 'password'})</userinput>
<userinput>box.schema.user.grant('replication','read,write','universe')</userinput>
<userinput>box.space._cluster:select({0},{iterator='GE'})</userinput>
</programlisting>
</para>
<para>
The result is that a new cluster is set up, and
the UUID is displayed.
Now the screen looks like this: (except that UUID values are always different):
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>$ <userinput># Terminal 1</userinput>
$ <userinput>mkdir -p ~/tarantool_test_node_1</userinput>
$ <userinput>cd ~/tarantool_test_node_1</userinput>
~/tarantool_test_node_1$ <userinput>rm -R ~/tarantool_test_node_1/*</userinput>
~/tarantool_test_node_1$ <userinput>~/tarantool-master/src/tarantool</userinput>
~/tarantool-master/src/tarantool: version 1.6.3-1724-g033ed69
type 'help' for interactive help
tarantool&gt; <userinput>box.cfg{listen=3301}</userinput>
... ...
tarantool&gt; <userinput>box.schema.user.create('replication', {password = 'password'})</userinput>
2014-10-13 11:12:56.052 [25018] wal I> creating `./00000000000000000000.xlog.inprogress'
---
...
tarantool&gt; <userinput>box.schema.user.grant('replication','read,write','universe')</userinput>
---
...
tarantool&gt; <userinput>box.space._cluster:select({0},{iterator='GE'})</userinput>
---
- - [1, '6190d919-1133-4452-b123-beca0b178b32']
...
</programlisting></entry>
          <entry><programlisting>$                              














</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

On the second shell, which we'll call Terminal #2,
execute these commands:<programlisting>
<userinput># Terminal 2</userinput>
<userinput>mkdir -p ~/tarantool_test_node_2</userinput>
<userinput>cd ~/tarantool_test_node_2</userinput>
<userinput>rm -R ~/tarantool_test_node_2/*</userinput>
<userinput>~/tarantool-master/src/tarantool</userinput>
<userinput>box.cfg{listen=3302, replication_source='replication:password@localhost:3301'}</userinput>
<userinput>box.space._cluster:select({0},{iterator='GE'})</userinput></programlisting>
The result is that a replica is set up.
Messages appear on Terminal #1 confirming that the
replica has connected and that the WAL contents
have been shipped to the replica.
Messages appear on Terminal #2 showing that
replication is starting.
Also on Terminal#2 the _cluster UUID value is displayed, and it is
the same as the _cluster UUID value that
was displayed on Terminal #1, because both
servers are in the same cluster.

    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; box.space._cluster:select({0},{iterator='GE'})
---
- - [1, '6190d919-1133-4452-b123-beca0b178b32']
...
tarantool&gt; 2014-10-13 11:20:08.691 [25020] main/101/spawner I> created a replication relay: pid = 25583
2014-10-13 11:20:08.691 [25583] main/101/relay/127.0.0.1:50883 I> recovery start
2014-10-13 11:20:08.691 [25583] main/101/relay/127.0.0.1:50883 I> recovering from `./00000000000000000000.snap'
2014-10-13 11:20:08.692 [25583] main/101/relay/127.0.0.1:50883 I> snapshot sent
2014-10-13 11:20:08.789 [25020] main/101/spawner I> created a replication relay: pid = 25585
2014-10-13 11:20:08.890 [25585] main/101/relay/127.0.0.1:50884 I> recover from `./00000000000000000000.xlog'
</programlisting></entry>
<entry><programlisting><prompt>$</prompt> <userinput># Terminal 2</userinput>
~/tarantool_test_node_2$ <userinput>mkdir -p ~/tarantool_test_node_2</userinput>
~/tarantool_test_node_2$ <userinput>cd ~/tarantool_test_node_2</userinput>
~/tarantool_test_node_2$ <userinput>rm -R ~/tarantool_test_node_2/*</userinput>
~/tarantool_test_node_2$ <userinput>~/tarantool-master/src/tarantool</userinput>
/home/username/tarantool-master/src/tarantool: version 1.6.3-1724-g033ed69
type 'help' for interactive help
tarantool&gt; <userinput>box.cfg{listen=3302, replication_source='replication:password@localhost:3301'}</userinput>
... ...
---
...
tarantool&gt; <userinput>box.space._cluster:select({0},{iterator='GE'})</userinput>
2014-10-13 11:20:08.789 [25579] main/103/replica/localhost:3301 C> connected to 127.0.0.1:3301
2014-10-13 11:20:08.789 [25579] main/103/replica/localhost:3301 I> authenticated
2014-10-13 11:20:08.901 [25579] wal I> creating `./00000000000000000000.xlog.inprogress'
---
- - [1, '6190d919-1133-4452-b123-beca0b178b32']
  - [2, '236230b8-af3e-406b-b709-15a60b44c20c']
...</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

On Terminal #1, execute these requests:
<programlisting><userinput>s = box.schema.space.create('tester')</userinput>
<userinput>i = s:create_index('primary', {})</userinput>
<userinput>s:insert{1,'Tuple inserted on Terminal #1'}</userinput></programlisting>
Now the screen looks like this:
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; 2014-10-13 11:20:08.691 [25020] main/101/spawner I> created a replication relay: pid = 25583
2014-10-13 11:20:08.691 [25583] main/101/relay/127.0.0.1:50883 I> recovery start
2014-10-13 11:20:08.691 [25583] main/101/relay/127.0.0.1:50883 I> recovering from `./00000000000000000000.snap'
2014-10-13 11:20:08.692 [25583] main/101/relay/127.0.0.1:50883 I> snapshot sent
2014-10-13 11:20:08.789 [25020] main/101/spawner I> created a replication relay: pid = 25585
2014-10-13 11:20:08.890 [25585] main/101/relay/127.0.0.1:50884 I> recover from `./00000000000000000000.xlog'
---
...
tarantool&gt; <userinput>s = box.schema.space.create('tester')</userinput>
---
...
tarantool&gt; <userinput>i = s:create_index('primary', {})</userinput>
---
...
tarantool&gt; <userinput>s:insert{1,'Tuple inserted on Terminal #1'}</userinput>
---
- [1, 'Tuple inserted on Terminal #1']
...
</programlisting></entry>
          <entry><programlisting><prompt>$ # Terminal 2
~/tarantool_test_node_2$ mkdir -p ~/tarantool_test_node_2
~/tarantool_test_node_2$ cd ~/tarantool_test_node_2
~/tarantool_test_node_2$ rm -R ~/tarantool_test_node_2/*
~/tarantool_test_node_2$ ~/tarantool-master/src/tarantool
/home/username/tarantool-master/src/tarantool: version 1.6.3-1724-g033ed69
type 'help' for interactive help
tarantool&gt; box.cfg{listen=3302, replication_source='replication:password@localhost:3301'}
... ...
---
...
tarantool&gt; box.space._cluster:select({0},{iterator='GE'})
2014-10-13 11:20:08.789 [25579] main/103/replica/localhost:3301 C> connected to 127.0.0.1:3301
2014-10-13 11:20:08.789 [25579] main/103/replica/localhost:3301 I> authenticated
2014-10-13 11:20:08.901 [25579] wal I> creating `./00000000000000000000.xlog.inprogress'

---
- - [1, '6190d919-1133-4452-b123-beca0b178b32']
  - [2, '236230b8-af3e-406b-b709-15a60b44c20c']
...</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

The creation and insertion were successful on Terminal #1.
Nothing has happened on Terminal #2.
</para>
<para>
On Terminal #2, execute these requests:<programlisting>
<userinput>s = box.space.tester</userinput>
<userinput>s:select({1},{iterator='GE'})</userinput>
<userinput>s:insert{2,'Tuple inserted on Terminal #2'}</userinput></programlisting>
Now the screen looks like this:

    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting><prompt>...
tarantool&gt; 2014-10-13 11:20:08.691 [25020] main/101/spawner I> created a replication relay: pid = 25583
2014-10-13 11:20:08.691 [25583] main/101/relay/127.0.0.1:50883 I> recovery start
2014-10-13 11:20:08.691 [25583] main/101/relay/127.0.0.1:50883 I> recovering from `./00000000000000000000.snap'
2014-10-13 11:20:08.692 [25583] main/101/relay/127.0.0.1:50883 I> snapshot sent
2014-10-13 11:20:08.789 [25020] main/101/spawner I> created a replication relay: pid = 25585
2014-10-13 11:20:08.890 [25585] main/101/relay/127.0.0.1:50884 I> recover from `./00000000000000000000.xlog'
---
...
tarantool&gt; s = box.schema.space.create('tester')
---
...
tarantool&gt; i = s:create_index('primary', {})
---
...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...</prompt></programlisting></entry>
          <entry><programlisting>... ...
tarantool&gt; box.space._cluster:select({0},{iterator='GE'})
2014-10-13 11:20:08.789 [25579] main/103/replica/localhost:3301 C> connected to 127.0.0.1:3301
2014-10-13 11:20:08.789 [25579] main/103/replica/localhost:3301 I> authenticated
2014-10-13 11:20:08.901 [25579] wal I> creating `./00000000000000000000.xlog.inprogress'
---
- - [1, '6190d919-1133-4452-b123-beca0b178b32']
  - [2, '236230b8-af3e-406b-b709-15a60b44c20c']
...
tarantool&gt; <userinput>s = box.space.tester</userinput>
---
...
tarantool&gt; <userinput>s:select({1},{iterator='GE'})</userinput>
---
- - [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; <userinput>s:insert{2,'Tuple inserted on Terminal #2'}</userinput>
---
- [2, 'Tuple inserted on Terminal #2']
...
</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

The selection and insertion were successful on Terminal #2.
Nothing has happened on Terminal #1.
</para>
<para>
On Terminal #1, execute these Tarantool requests and shell commands:<programlisting>
<userinput>os.exit()</userinput>
<userinput>ls -l ~/tarantool_test_node_1</userinput>
<userinput>ls -l ~/tarantool_test_node_2</userinput></programlisting>
Now Tarantool #1 is stopped.
Messages appear on Terminal #2 announcing that fact.
The "ls -l" commands show that both servers have
made snapshots, which have the same size because
they both contain the same tuples.

    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; <userinput>os.exit()</userinput>
2014-10-13 11:45:20.455 [25585] main/101/relay/127.0.0.1:50884 I> done `./00000000000000000000.xlog'
2014-10-13 11:45:20.531 [25020] main/101/spawner I> Exiting: master shutdown
2014-10-13 11:45:20.531 [25020] main/101/spawner I> sending signal 15 to 1 children
2014-10-13 11:45:20.531 [25020] main/101/spawner I> waiting for children for up to 5 seconds
~/tarantool_test_node_1$ <userinput>ls -l ~/tarantool_test_node_1</userinput>
total 8
-rw-rw-r-- 1  1781 Oct 13 11:12 00000000000000000000.snap
-rw-rw-r-- 1   518 Oct 13 11:45 00000000000000000000.xlog
~/tarantool_test_node_1$ <userinput>ls -l ~/tarantool_test_node_2/</userinput>
total 8
-rw-rw-r-- 1  1781 Oct 13 11:20 00000000000000000000.snap
-rw-rw-r-- 1   588 Oct 13 11:38 00000000000000000000.xlog
~/tarantool_test_node_1$ 
</programlisting></entry>
          <entry><programlisting><prompt>... ...
tarantool&gt; s:select({1},{iterator='GE'})
---
- - [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> can't read row
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 10, aka 127.0.0.1:50884, peer of 127.0.0.1:3301: Broken pipe
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> will retry every 1 second
</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

On Terminal #2, ignore the repeated messages saying "failed to connect", and execute these requests:<programlisting>
<userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
<userinput>box.space.tester:insert{3,'Another'}</userinput></programlisting>
Now the screen looks like this (ignoring the repeated messages saying "failed to connect"):
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting><prompt>... ...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; os.exit()
2014-10-13 11:45:20.455 [25585] main/101/relay/127.0.0.1:50884 I> done `./00000000000000000000.xlog'
2014-10-13 11:45:20.531 [25020] main/101/spawner I> Exiting: master shutdown
2014-10-13 11:45:20.531 [25020] main/101/spawner I> sending signal 15 to 1 children
2014-10-13 11:45:20.531 [25020] main/101/spawner I> waiting for children for up to 5 seconds
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_1
total 8
-rw-rw-r-- 1  1781 Oct 13 11:12 00000000000000000000.snap
-rw-rw-r-- 1   518 Oct 13 11:45 00000000000000000000.xlog
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_2/
total 8
-rw-rw-r-- 1  1781 Oct 13 11:20 00000000000000000000.snap
-rw-rw-r-- 1   588 Oct 13 11:38 00000000000000000000.xlog
~/tarantool_test_node_1$ 
</prompt></programlisting></entry>
          <entry><programlisting>... ...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> can't read row
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 10, aka 127.0.0.1:50884, peer of 127.0.0.1:3301: Broken pipe
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> will retry every 1 second
tarantool&gt; <userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; <userinput>box.space.tester:insert{3,'Another'}</userinput>
---
- [3, 'Another']
...
</programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

Terminal #2 has done a select and an insert,
even though Terminal #1 is down.
</para>
<para>
On Terminal #1 execute these commands:<programlisting>
<userinput>~/tarantool-master/src/tarantool</userinput>
<userinput>box.cfg{listen=3301}</userinput>
<userinput>box.space.tester:select({0},{iterator='GE'})</userinput></programlisting>
Now the screen looks like this (ignoring the repeated messages on terminal #2 saying "failed to connect"):
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
tarantool&gt; s:insert{1,'Tuple inserted on Terminal #1'}
---
- [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; os.exit()
2014-10-13 11:45:20.455 [25585] main/101/relay/127.0.0.1:50884 I> done `./00000000000000000000.xlog'
2014-10-13 11:45:20.531 [25020] main/101/spawner I> Exiting: master shutdown
2014-10-13 11:45:20.531 [25020] main/101/spawner I> sending signal 15 to 1 children
2014-10-13 11:45:20.531 [25020] main/101/spawner I> waiting for children for up to 5 seconds
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_1
total 8
-rw-rw-r-- 1  1781 Oct 13 11:12 00000000000000000000.snap
-rw-rw-r-- 1   518 Oct 13 11:45 00000000000000000000.xlog
~/tarantool_test_node_1$ ls -l ~/tarantool_test_node_2/
total 8
-rw-rw-r-- 1  1781 Oct 13 11:20 00000000000000000000.snap
-rw-rw-r-- 1   588 Oct 13 11:38 00000000000000000000.xlog
~/tarantool_test_node_1$ <userinput>~/tarantool-master/src/tarantool</userinput>
/home/username/tarantool-master/src/tarantool: version 1.6.3-515-g0a06cce
type 'help' for interactive help
tarantool&gt; <userinput>box.cfg{listen=3301}</userinput>
... ...
---
...
tarantool&gt; <userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
2014-10-13 12:01:55.615 [28989] main/101/spawner I> created a replication relay: pid = 28992
2014-10-13 12:01:55.716 [28992] main/101/relay/127.0.0.1:51892 I> recover from `./00000000000000000000.xlog'
2014-10-13 12:01:55.716 [28992] main/101/relay/127.0.0.1:51892 I> done `./00000000000000000000.xlog'
---
- - [1, 'Tuple inserted on Terminal #1']
...
</programlisting></entry>
          <entry><programlisting><prompt>... ...
tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> can't read row
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 10, aka 127.0.0.1:50884, peer of 127.0.0.1:3301: Broken pipe
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> will retry every 1 second
tarantool&gt; box.space.tester:select({0},{iterator='GE'})
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; box.space.tester:insert{3,'Another'}
---
- [3, 'Another']
...
tarantool&gt;
2014-10-13 12:01:55.614 [25579] main/103/replica/localhost:3301 C> connected to 127.0.0.1:3301
2014-10-13 12:01:55.614 [25579] main/103/replica/localhost:3301 I> authenticated
</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>


The master has reconnected to the cluster,
and has NOT found what the replica wrote
while the master was away. That is not a
surprise -- the replica has not been asked
to act as a replication source.
</para>
<para>
On Terminal #1, say:<programlisting>
<userinput>box.cfg{replication_source='replication:password@localhost:3302'}</userinput>
<userinput>box.space.tester:select({0},{iterator='GE'})</userinput></programlisting>
The screen now looks like this:
    <informaltable>
    <tgroup cols="2" align="left" colsep="1" rowsep="0">
     <thead>
      <row><entry align="center">TERMINAL #1</entry><entry align="center">TERMINAL #2</entry></row>
     </thead>
    <tbody>
     <row><entry><programlisting>... ...
~/tarantool_test_node_1$ ~/tarantool-master/src/tarantool
~/tarantool: version 1.6.3-1724-g033ed69
type 'help' for interactive help
tarantool&gt; box.cfg{listen=3301}
... ...
---
...
tarantool&gt; box.space.tester:select({0},{iterator='GE'})
2014-10-13 12:01:55.615 [28989] main/101/spawner I> created a replication relay: pid = 28992
2014-10-13 12:01:55.716 [28992] main/101/relay/127.0.0.1:51892 I> recover from `./00000000000000000000.xlog'
2014-10-13 12:01:55.716 [28992] main/101/relay/127.0.0.1:51892 I> done `./00000000000000000000.xlog'

---
- - [1, 'Tuple inserted on Terminal #1']
...
tarantool&gt; <userinput>box.cfg{replication_source='replication:password@localhost:3302'}</userinput>
2014-10-13 12:10:21.485 [28987] main/101/interactive C> starting replication from localhost:3302
---
...
2014-10-13 12:10:21.487 [28987] main/104/replica/localhost:3302 C> connected to 127.0.0.1:3302
2014-10-13 12:10:21.487 [28987] main/104/replica/localhost:3302 I> authenticated
tarantool&gt; <userinput>box.space.tester:select({0},{iterator='GE'})</userinput>
2014-10-13 12:10:21.592 [28987] wal I> creating `./00000000000000000006.xlog.inprogress'
2014-10-13 12:10:21.617 [28992] main/101/relay/127.0.0.1:51892 I> recover from `./00000000000000000006.xlog'
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
  - [3, 'Another']
...
</programlisting></entry>
          <entry><programlisting><prompt>... ...
           tarantool&gt; s:insert{2,'Tuple inserted on Terminal #2'}
---
- [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; 2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> can't read row
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 !> SystemError
unexpected EOF when reading from socket,
called on fd 10, aka 127.0.0.1:50884, peer of 127.0.0.1:3301: Broken pipe
2014-10-13 11:45:20.532 [25579] main/103/replica/localhost:3301 I> will retry every 1 second
tarantool&gt; box.space.tester:select({0},{iterator='GE'})
---
- - [1, 'Tuple inserted on Terminal #1']
  - [2, 'Tuple inserted on Terminal #2']
...
tarantool&gt; box.space.tester:insert{3,'Another'}
---
- [3, 'Another']
...
tarantool&gt;
2014-10-13 12:01:55.614 [25579] main/103/replica/localhost:3301 C> connected to 127.0.0.1:3301
2014-10-13 12:01:55.614 [25579] main/103/replica/localhost:3301 I> authenticated
2014-10-13 12:10:21.488 [25581] main/101/spawner I> created a replication relay: pid = 29632
2014-10-13 12:10:21.592 [29632] main/101/relay/127.0.0.1:45908 I> recover from `./00000000000000000000.xlog'
</prompt></programlisting></entry></row>
    </tbody>
    </tgroup>                                   
    </informaltable>

This shows that the two servers are once
again in synch, and that each server sees
what the other server wrote.
</para>
<para>
To clean up, say "os.exit()" on both
Terminal #1 and Terminal #2, and then
on either terminal say:<programlisting>
<userinput>cd ~</userinput>
<userinput>rm -R ~/tarantool_test_node_1</userinput>
<userinput>rm -R ~/tarantool_test_node_2</userinput></programlisting>
</para>


</section>

</chapter>

<!--
vim: tw=66 syntax=docbk
vim: spell spelllang=en_us
-->
